{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = np.random.permutation(indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    t = jnp.linspace(0, 2 * math.pi, 16)\n",
    "    offset = jrandom.uniform(key, (dataset_size, 1), minval=0, maxval=2 * math.pi)\n",
    "    x1 = jnp.sin(t + offset) / (1 + t)\n",
    "    x2 = jnp.cos(t + offset) / (1 + t)\n",
    "    y = jnp.ones((dataset_size, 1))\n",
    "\n",
    "    half_dataset_size = dataset_size // 2\n",
    "    x1 = x1.at[:half_dataset_size].multiply(-1)\n",
    "    y = y.at[:half_dataset_size].set(0)\n",
    "    x = jnp.stack([x1, x2], axis=-1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    cell: eqx.Module\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_size, *, key):\n",
    "        ckey, lkey = jrandom.split(key)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = eqx.nn.GRUCell(in_size, hidden_size, key=ckey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "\n",
    "        def f(carry, inp):\n",
    "            return self.cell(inp, carry), None\n",
    "\n",
    "        out, _ = lax.scan(f, hidden, input)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        return jax.nn.sigmoid(self.linear(out) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=10000,\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-3,\n",
    "    steps=200,\n",
    "    hidden_size=16,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    data_key, model_key = jrandom.split(jrandom.PRNGKey(seed), 2)\n",
    "    xs, ys = get_data(dataset_size, key=data_key)\n",
    "    iter_data = dataloader((xs, ys), batch_size)\n",
    "\n",
    "    model = RNN(in_size=2, out_size=1, hidden_size=hidden_size, key=model_key)\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def compute_loss(model, x, y):\n",
    "        pred_y = jax.vmap(model)(x)\n",
    "        # Trains with respect to binary cross-entropy\n",
    "        return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n",
    "\n",
    "    # Important for efficiency whenever you use JAX: wrap everything into a single JIT\n",
    "    # region.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, x, y, opt_state):\n",
    "        loss, grads = compute_loss(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    optim = optax.adam(learning_rate)\n",
    "    opt_state = optim.init(model)\n",
    "    for step, (x, y) in zip(range(steps), iter_data):\n",
    "        loss, model, opt_state = make_step(model, x, y, opt_state)\n",
    "        loss = loss.item()\n",
    "        print(f\"step={step}, loss={loss}\")\n",
    "\n",
    "    pred_ys = jax.vmap(model)(xs)\n",
    "    num_correct = jnp.sum((pred_ys > 0.5) == ys)\n",
    "    final_accuracy = (num_correct / dataset_size).item()\n",
    "    print(f\"final_accuracy={final_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=0.7041164040565491\n",
      "step=1, loss=0.6740140914916992\n",
      "step=2, loss=0.7094993591308594\n",
      "step=3, loss=0.7015631794929504\n",
      "step=4, loss=0.6975655555725098\n",
      "step=5, loss=0.7034317255020142\n",
      "step=6, loss=0.6803090572357178\n",
      "step=7, loss=0.6948286890983582\n",
      "step=8, loss=0.6797491908073425\n",
      "step=9, loss=0.6956168413162231\n",
      "step=10, loss=0.6945274472236633\n",
      "step=11, loss=0.6890615224838257\n",
      "step=12, loss=0.7001497745513916\n",
      "step=13, loss=0.6958308219909668\n",
      "step=14, loss=0.6998479962348938\n",
      "step=15, loss=0.6900094747543335\n",
      "step=16, loss=0.6902174353599548\n",
      "step=17, loss=0.6975182294845581\n",
      "step=18, loss=0.692755937576294\n",
      "step=19, loss=0.6899957656860352\n",
      "step=20, loss=0.6916804313659668\n",
      "step=21, loss=0.6952881217002869\n",
      "step=22, loss=0.6952449083328247\n",
      "step=23, loss=0.6966341137886047\n",
      "step=24, loss=0.6946205496788025\n",
      "step=25, loss=0.6959720849990845\n",
      "step=26, loss=0.6943169236183167\n",
      "step=27, loss=0.693242073059082\n",
      "step=28, loss=0.6943486928939819\n",
      "step=29, loss=0.6947110891342163\n",
      "step=30, loss=0.6933777928352356\n",
      "step=31, loss=0.6932740211486816\n",
      "step=32, loss=0.6927382946014404\n",
      "step=33, loss=0.6929991841316223\n",
      "step=34, loss=0.6920375823974609\n",
      "step=35, loss=0.692162036895752\n",
      "step=36, loss=0.6934462785720825\n",
      "step=37, loss=0.6917566657066345\n",
      "step=38, loss=0.6926522254943848\n",
      "step=39, loss=0.6932585835456848\n",
      "step=40, loss=0.6917635202407837\n",
      "step=41, loss=0.6927347183227539\n",
      "step=42, loss=0.6959287524223328\n",
      "step=43, loss=0.6911413073539734\n",
      "step=44, loss=0.693894624710083\n",
      "step=45, loss=0.6928430795669556\n",
      "step=46, loss=0.6933132410049438\n",
      "step=47, loss=0.6889058351516724\n",
      "step=48, loss=0.6933602690696716\n",
      "step=49, loss=0.6931872963905334\n",
      "step=50, loss=0.692786455154419\n",
      "step=51, loss=0.692165732383728\n",
      "step=52, loss=0.6936808824539185\n",
      "step=53, loss=0.6916918158531189\n",
      "step=54, loss=0.6910754442214966\n",
      "step=55, loss=0.6925906538963318\n",
      "step=56, loss=0.6915077567100525\n",
      "step=57, loss=0.6931888461112976\n",
      "step=58, loss=0.6955764889717102\n",
      "step=59, loss=0.6969563364982605\n",
      "step=60, loss=0.6932373046875\n",
      "step=61, loss=0.6932051181793213\n",
      "step=62, loss=0.6930014491081238\n",
      "step=63, loss=0.6932759881019592\n",
      "step=64, loss=0.6913470029830933\n",
      "step=65, loss=0.6948730945587158\n",
      "step=66, loss=0.6913539171218872\n",
      "step=67, loss=0.6871986389160156\n",
      "step=68, loss=0.6943439841270447\n",
      "step=69, loss=0.6830768585205078\n",
      "step=70, loss=0.6907325983047485\n",
      "step=71, loss=0.6950952410697937\n",
      "step=72, loss=0.698269248008728\n",
      "step=73, loss=0.6998335123062134\n",
      "step=74, loss=0.6767249703407288\n",
      "step=75, loss=0.6940581798553467\n",
      "step=76, loss=0.6886690855026245\n",
      "step=77, loss=0.6858627796173096\n",
      "step=78, loss=0.7053946256637573\n",
      "step=79, loss=0.690345823764801\n",
      "step=80, loss=0.7014570236206055\n",
      "step=81, loss=0.7001966238021851\n",
      "step=82, loss=0.6820825338363647\n",
      "step=83, loss=0.6776625514030457\n",
      "step=84, loss=0.7027115821838379\n",
      "step=85, loss=0.7082402110099792\n",
      "step=86, loss=0.6824586391448975\n",
      "step=87, loss=0.7107756733894348\n",
      "step=88, loss=0.6938910484313965\n",
      "step=89, loss=0.6971842050552368\n",
      "step=90, loss=0.6862659454345703\n",
      "step=91, loss=0.700100839138031\n",
      "step=92, loss=0.6888860464096069\n",
      "step=93, loss=0.6860650181770325\n",
      "step=94, loss=0.6776611804962158\n",
      "step=95, loss=0.6949869394302368\n",
      "step=96, loss=0.7023707628250122\n",
      "step=97, loss=0.6936022043228149\n",
      "step=98, loss=0.692290186882019\n",
      "step=99, loss=0.6978790760040283\n",
      "step=100, loss=0.6842526197433472\n",
      "step=101, loss=0.6903141736984253\n",
      "step=102, loss=0.693016767501831\n",
      "step=103, loss=0.6874117255210876\n",
      "step=104, loss=0.6946340799331665\n",
      "step=105, loss=0.6953887343406677\n",
      "step=106, loss=0.6932365298271179\n",
      "step=107, loss=0.695623517036438\n",
      "step=108, loss=0.6907258629798889\n",
      "step=109, loss=0.6912298202514648\n",
      "step=110, loss=0.689899206161499\n",
      "step=111, loss=0.6906846761703491\n",
      "step=112, loss=0.6894844770431519\n",
      "step=113, loss=0.7000057697296143\n",
      "step=114, loss=0.689942479133606\n",
      "step=115, loss=0.687817394733429\n",
      "step=116, loss=0.691552460193634\n",
      "step=117, loss=0.6871337890625\n",
      "step=118, loss=0.6876622438430786\n",
      "step=119, loss=0.6891277432441711\n",
      "step=120, loss=0.6908859610557556\n",
      "step=121, loss=0.6846441030502319\n",
      "step=122, loss=0.7003451585769653\n",
      "step=123, loss=0.6967604160308838\n",
      "step=124, loss=0.6874613761901855\n",
      "step=125, loss=0.6806151270866394\n",
      "step=126, loss=0.6786460280418396\n",
      "step=127, loss=0.6835008859634399\n",
      "step=128, loss=0.6774575710296631\n",
      "step=129, loss=0.6797239184379578\n",
      "step=130, loss=0.6781196594238281\n",
      "step=131, loss=0.6726813912391663\n",
      "step=132, loss=0.6690242290496826\n",
      "step=133, loss=0.6721709966659546\n",
      "step=134, loss=0.665722668170929\n",
      "step=135, loss=0.6553646326065063\n",
      "step=136, loss=0.6473804712295532\n",
      "step=137, loss=0.6342117786407471\n",
      "step=138, loss=0.6330440044403076\n",
      "step=139, loss=0.6101658344268799\n",
      "step=140, loss=0.5745416879653931\n",
      "step=141, loss=0.5818763971328735\n",
      "step=142, loss=0.5640103816986084\n",
      "step=143, loss=0.5327913761138916\n",
      "step=144, loss=0.484192430973053\n",
      "step=145, loss=0.3978121876716614\n",
      "step=146, loss=0.37542295455932617\n",
      "step=147, loss=0.35479456186294556\n",
      "step=148, loss=0.29561981558799744\n",
      "step=149, loss=0.25019484758377075\n",
      "step=150, loss=0.2032475769519806\n",
      "step=151, loss=0.16052491962909698\n",
      "step=152, loss=0.1372300386428833\n",
      "step=153, loss=0.11687446385622025\n",
      "step=154, loss=0.09571731835603714\n",
      "step=155, loss=0.08343568444252014\n",
      "step=156, loss=0.06993676722049713\n",
      "step=157, loss=0.06075170636177063\n",
      "step=158, loss=0.051666419953107834\n",
      "step=159, loss=0.04486401751637459\n",
      "step=160, loss=0.03933790326118469\n",
      "step=161, loss=0.03540056198835373\n",
      "step=162, loss=0.030756060034036636\n",
      "step=163, loss=0.027269184589385986\n",
      "step=164, loss=0.024746742099523544\n",
      "step=165, loss=0.0835975632071495\n",
      "step=166, loss=0.020985975861549377\n",
      "step=167, loss=0.019550379365682602\n",
      "step=168, loss=0.01888895034790039\n",
      "step=169, loss=0.017341583967208862\n",
      "step=170, loss=0.019057584926486015\n",
      "step=171, loss=0.10960463434457779\n",
      "step=172, loss=0.023587726056575775\n",
      "step=173, loss=0.1719374656677246\n",
      "step=174, loss=0.015328864566981792\n",
      "step=175, loss=0.014757741242647171\n",
      "step=176, loss=0.013272088021039963\n",
      "step=177, loss=0.012922447174787521\n",
      "step=178, loss=0.01220858097076416\n",
      "step=179, loss=0.011479915119707584\n",
      "step=180, loss=0.011850135400891304\n",
      "step=181, loss=0.01121574454009533\n",
      "step=182, loss=0.011140652932226658\n",
      "step=183, loss=0.010780637152493\n",
      "step=184, loss=0.010651616379618645\n",
      "step=185, loss=0.010467308573424816\n",
      "step=186, loss=0.010288134217262268\n",
      "step=187, loss=0.010086576454341412\n",
      "step=188, loss=0.009904627688229084\n",
      "step=189, loss=0.009952813386917114\n",
      "step=190, loss=0.009561052545905113\n",
      "step=191, loss=0.009201761335134506\n",
      "step=192, loss=0.009461009874939919\n",
      "step=193, loss=0.009307075291872025\n",
      "step=194, loss=0.00885889120399952\n",
      "step=195, loss=0.00867369957268238\n",
      "step=196, loss=0.009152747690677643\n",
      "step=197, loss=0.009678962640464306\n",
      "step=198, loss=0.009228616952896118\n",
      "step=199, loss=0.008974550291895866\n",
      "final_accuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "main()  # All right, let's run the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
